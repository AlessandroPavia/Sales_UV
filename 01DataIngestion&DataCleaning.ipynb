{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f397b84",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b43e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import Levenshtein as lev\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d4765d",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3593a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abeac0",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d114ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_fuzz(df_temp, colonna, soglia_fuzz):\n",
    "    '''\n",
    "    Applies fuzzy matching to strings in a specified DataFrame column by comparing their uppercase forms. \n",
    "    If the similarity (via Levenshtein distance using `fuzz.ratio`) exceeds a given threshold, \n",
    "    the less frequent string is replaced with the more frequent one.\n",
    "\n",
    "    Parameters:\n",
    "    df_temp : DataFrame\n",
    "        The DataFrame containing the column with the strings to analyze.\n",
    "    colonna : str\n",
    "        The name of the column in which the string values are compared and modified.\n",
    "    soglia_fuzz : int\n",
    "        The similarity threshold above which two strings are considered similar \n",
    "        enough to be grouped.\n",
    "\n",
    "    Returns:\n",
    "    df_temp2 : DataFrame\n",
    "        A copy of the original DataFrame, with an additional column '<colonna>_mod' containing \n",
    "        the cleaned and standardized string values.\n",
    "    varianti_dict : dict\n",
    "        A dictionary where each key is a standardized string (the final form), and the corresponding \n",
    "        value is a list of original string variants that have been grouped under that key.\n",
    "    '''\n",
    "    df_temp2=df_temp.copy() #poi usare df_temp2\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "\n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "\n",
    "    diz = df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1 != val2:\n",
    "                similarità = fuzz.ratio(val1, val2)\n",
    "                if similarità >= soglia_fuzz and diz[val1] >= diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1  #Sostituisci tutti i valori uguali a val2 nella colonna colonna_mod del DataFrame df_temp2 con val1.# creare nuova colonna invece di replace\n",
    "                elif similarità >=soglia_fuzz and diz[val1]<diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "   \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    #print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    varianti_dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "\n",
    "     \n",
    "\n",
    "    return df_temp2, varianti_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb44fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_fuzz2(df_temp, colonna, soglia_fuzz,colonna_da_uguagliare):\n",
    "    '''\n",
    "    Applies fuzzy matching to strings in a specified DataFrame column by comparing their uppercase forms. \n",
    "    If the similarity (via Levenshtein distance using `fuzz.ratio`) exceeds a given threshold and the strings in colonna_da_uguagliare are the same\n",
    "    the less frequent string is replaced with the more frequent one.\n",
    "\n",
    "    Parameters:\n",
    "    df_temp : DataFrame\n",
    "        The DataFrame containing the column with the strings to analyze.\n",
    "    colonna : str\n",
    "        The name of the column in which the string values are compared and modified.\n",
    "    soglia_fuzz : int\n",
    "        The similarity threshold above which two strings are considered similar \n",
    "        enough to be grouped.\n",
    "    colonna_da_uguagliare: str\n",
    "        The name of the column in which the strings have to be the same in order to modify one of them--> it's going to be the column of the state\n",
    "\n",
    "    Returns:\n",
    "    df_temp2 : DataFrame\n",
    "        A copy of the original DataFrame, with an additional column '<colonna>_mod' containing \n",
    "        the cleaned and standardized string values.\n",
    "    varianti_dict : dict\n",
    "        A dictionary where each key is a standardized string (the final form), and the corresponding \n",
    "        value is a list of original string variants that have been grouped under that key.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_temp2=df_temp.copy() #poi usare df_temp2\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "\n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "    diz = df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1 != val2:\n",
    "                similarità = fuzz.ratio(val1, val2)\n",
    "                if similarità >= soglia_fuzz and diz[val1] >= diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1  #Sostituisci tutti i valori uguali a val2 nella colonna colonna_mod del DataFrame df_temp2 con val1.# creare nuova colonna invece di replace\n",
    "                elif similarità >=soglia_fuzz and diz[val1]<diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "   \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    #print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    varianti_dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "     \n",
    "\n",
    "    return df_temp2, varianti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade43419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_lev(df_temp,colonna,soglia_lev):\n",
    "    '''\n",
    "    Applies Levenshtein distance to strings in a specified DataFrame column by comparing their uppercase forms. \n",
    "    If the distance exceeds a given threshold, the less frequent string is replaced with the more frequent one.\n",
    "\n",
    "    Parameters:\n",
    "    df_temp : DataFrame\n",
    "        The DataFrame containing the column with the strings to analyze.\n",
    "    colonna : str\n",
    "        The name of the column in which the string values are compared and modified.\n",
    "    soglia_lev : int\n",
    "        The distance threshold above which two strings are considered similar \n",
    "        enough to be grouped.\n",
    "\n",
    "    Returns:\n",
    "    df_temp2 : DataFrame\n",
    "        A copy of the original DataFrame, with an additional column '<colonna>_mod' containing \n",
    "        the cleaned and standardized string values.\n",
    "    varianti_dict : dict\n",
    "        A dictionary where each key is a standardized string (the final form), and the corresponding \n",
    "        value is a list of original string variants that have been grouped under that key.\n",
    "    '''\n",
    "    df_temp2=df_temp.copy()\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "    \n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "    diz= df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1!=val2:\n",
    "                distanza=lev.distance(val1,val2)\n",
    "                if distanza <= soglia_lev and diz[val1]>=diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1\n",
    "                elif distanza <= soglia_lev and diz[val1]<diz[val2]:\n",
    "                    df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "    \n",
    "    \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    #print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    varianti_dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "\n",
    "\n",
    "    return df_temp2, varianti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_stringhe_lev2(df_temp,colonna,soglia_lev,colonna_da_uguagliare):\n",
    "    '''\n",
    "    Applies Levenshtein distance to strings in a specified DataFrame column by comparing their uppercase forms. \n",
    "    If the distance exceeds a given threshold and the strings in colonna_da_uguagliare are the same, the less frequent string is replaced with the more frequent one.\n",
    "\n",
    "    Parameters:\n",
    "    df_temp : DataFrame\n",
    "        The DataFrame containing the column with the strings to analyze.\n",
    "    colonna : str\n",
    "        The name of the column in which the string values are compared and modified.\n",
    "    soglia_lev : int\n",
    "        The distance threshold above which two strings are considered similar \n",
    "        enough to be grouped.\n",
    "    colonna_da_uguagliare: The name of the column in which the strings have to be the same in order to modify one of them--> it's going to be the column of the state\n",
    "\n",
    "    Returns:\n",
    "    df_temp2 : DataFrame\n",
    "        A copy of the original DataFrame, with an additional column '<colonna>_mod' containing \n",
    "        the cleaned and standardized string values.\n",
    "    varianti_dict : dict\n",
    "        A dictionary where each key is a standardized string (the final form), and the corresponding \n",
    "        value is a list of original string variants that have been grouped under that key.\n",
    "    '''\n",
    "    df_temp2=df_temp.copy()\n",
    "    colonna_mod = f'{colonna}_mod'\n",
    "    df_temp2[colonna_mod] = df_temp2[colonna].str.upper()\n",
    "    \n",
    "\n",
    "    prima= set(df_temp2[colonna_mod].unique())\n",
    "    diz= df_temp2[colonna_mod].value_counts().to_dict()\n",
    "\n",
    "\n",
    "    for val1 in diz:\n",
    "        for val2 in diz:\n",
    "            if val1!=val2:\n",
    "                distanza=lev.distance(val1,val2)\n",
    "                if distanza <= soglia_lev and diz[val1]>=diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_mod] = val1 \n",
    "                elif distanza <= soglia_lev and diz[val1]<diz[val2]:\n",
    "                    stato_val1 = df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_da_uguagliare].iloc[0]\n",
    "                    stato_val2 = df_temp2.loc[df_temp2[colonna_mod] == val2, colonna_da_uguagliare].iloc[0]\n",
    "                    if stato_val1 == stato_val2:\n",
    "                        df_temp2.loc[df_temp2[colonna_mod] == val1, colonna_mod] = val2\n",
    "\n",
    "    \n",
    "    \n",
    "    dopo= set(df_temp2[colonna_mod].unique())\n",
    "    mancanti = prima - dopo\n",
    "    #print(\"Valori rimossi o modificati:\", mancanti, len(mancanti))\n",
    "\n",
    "    varianti_dict = {\n",
    "    key: sorted(set(s.upper() for s in group[colonna].unique() if isinstance(s, str)))\n",
    "    for key, group in df_temp2.groupby(colonna_mod)}\n",
    "\n",
    "    return df_temp2, varianti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5901e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pulisci_trattino(val):\n",
    "    '''\n",
    "    Replaces hyphens ('-') that are not at the beginning of the string with a dot ('.').\n",
    "\n",
    "    Parameters:\n",
    "    val: The input value, which will be converted to a string if not already.\n",
    "\n",
    "    Returns:\n",
    "    val: a string where all hyphens not at the start of the string have been replaced with dots.\n",
    "    \n",
    "    Notes:\n",
    "    The regular expression `(?<!^)-` is a negative lookbehind assertion that ensures\n",
    "    the hyphen is not at the start of the string before performing the substitution.\n",
    "    '''\n",
    "    val = str(val)\n",
    "    val = re.sub(r'(?<!^)-', '.', val)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d9758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creo_dict(df,colonna_originale,colonna_modificata):\n",
    "    '''\n",
    "Creates a dictionary that maps each unique value in the modified column \n",
    "to a list of unique values from the original column that correspond to it.\n",
    "\n",
    "Parameters:\n",
    "- df: The input DataFrame containing the data.\n",
    "- colonna_originale: The name of the column with the original, unmodified values.\n",
    "- colonna_modificata: The name of the column with the processed or normalized values.\n",
    "\n",
    "Returns:\n",
    "- A dictionary where each key is a unique value from the modified column, and each value \n",
    "    is a list of unique original values grouped under that key.\n",
    "    '''\n",
    "\n",
    "    dizionario={key: list(group[colonna_originale].unique())\n",
    "    for key, group in df.groupby(colonna_modificata)}\n",
    "    return dizionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95339bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creo_dict2(df, colonna_originale, colonna_modificata):\n",
    "    '''\n",
    "Generates a dictionary that maps each unique value in the modified column, \n",
    "rounded to two decimal places, to a list of unique corresponding values from the original column. \n",
    "It ensures numerical values are handled properly and preserves non-numeric values as they are.\n",
    "\n",
    "Parameters:\n",
    "- df: The input DataFrame containing the data.\n",
    "- colonna_originale: The name of the column with the original values.\n",
    "- colonna_modificata: The name of the column with the modified values to group by.\n",
    "\n",
    "Returns:\n",
    "- A dictionary where each key is a rounded (to 2 decimals) unique value from the modified column, \n",
    "  and the value is a list of unique corresponding entries from the original column.\n",
    "'''\n",
    "\n",
    "    dizionario = {\n",
    "        round(key, 2): list(set(\n",
    "            v if isinstance(v, (int, float)) else v\n",
    "            for v in group[colonna_originale]\n",
    "        ))\n",
    "        for key, group in df.groupby(colonna_modificata)\n",
    "    }\n",
    "    return dizionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22436a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniforma_date_order_date(df, colonna_originale1, colonna_originale2):\n",
    "    '''\n",
    "Standardizes and corrects date values in a specified column by attempting to convert \n",
    "them into a consistent datetime format (YYYY/MM/DD). It creates two new columns: one in datetime \n",
    "format and one with the final corrected values.\n",
    "\n",
    "For rows where the initial conversion fails (resulting in NaT), the function generates all possible \n",
    "permutations of the date components (e.g., day, month, year), tries to parse each permutation as \n",
    "a valid date, and selects the closest valid date that precedes or matches the date in a reference \n",
    "column (typically a shipment date).\n",
    "\n",
    "Parameters:\n",
    "- df: The input DataFrame.\n",
    "- colonna_originale1: Name of the column containing potentially messy date values (e.g., order dates).\n",
    "- colonna_originale2: Name of the reference date column to ensure logical ordering (e.g., ship dates).\n",
    "\n",
    "Returns:\n",
    "- df: The modified DataFrame, with two additional columns:\n",
    "    - '<colonna_originale1>_dt': the parsed datetime column from colonna_originale1.\n",
    "    - '<colonna_originale1>_def': the final corrected and standardized datetime values\n",
    "\n",
    "    '''\n",
    "    colonna1_dt=colonna_originale1 + '_dt'\n",
    "    colonna2_dt=colonna_originale2 + '_dt'\n",
    "    colonna1_def=colonna_originale1 + '_def'\n",
    "\n",
    "\n",
    "    df[colonna1_dt] = pd.to_datetime(df[colonna_originale1], format=\"%d/%m/%Y\",errors='coerce')\n",
    "    df[colonna2_dt] = pd.to_datetime(df[colonna_originale2], format=\"%d/%m/%Y\",errors='coerce')\n",
    "    \n",
    "    \n",
    "\n",
    "    #Inizializzo colonna finale\n",
    "    df[colonna1_def] = df[colonna1_dt]  # dove non è NaT, la data è già buona\n",
    "\n",
    "    \n",
    "    for i, row in df[pd.isna(df[colonna1_dt])].iterrows():\n",
    "        raw_date = str(row[colonna_originale1]) #rendo la data in stringa così posso lavorarci\n",
    "        display(f'La data iniziale è {raw_date}')\n",
    "        display(f'La shipping date è {row[colonna2_dt]}')\n",
    "        try:\n",
    "            parts = re.split(r'\\D+', raw_date.strip())\n",
    "            display(f'Le parti sono {parts}')\n",
    "             #cosi faccio lo split non solo sullo / ma qualsiasi simbolo non numerico\n",
    "            combinazioni = ['/'.join(p) for p in permutations(parts)] #trovo tutte le combinazioni possibili\n",
    "            display(f'Le combinazioni sono {combinazioni}')\n",
    "            date_possibili = pd.to_datetime(combinazioni, errors='coerce') #rendo ogni combinazione una data in formato dt\n",
    "            display(f'Le date possibili sono {date_possibili}')\n",
    "            date_valide = date_possibili[date_possibili.notna() & (date_possibili <= row[colonna2_dt])] #controllo che la data possibile sia precedente alla data di spedizione\n",
    "            display(f'Le date valide sono {date_valide}')\n",
    "            if not date_valide.empty:\n",
    "                distanze = row[colonna2_dt] - date_valide  # sarà sempre >= 0\n",
    "                display(distanze)\n",
    "                indice_minimo = distanze.argmin()\n",
    "                display(indice_minimo)\n",
    "                data_probabile = date_valide.iloc[indice_minimo] #indice della differenza minima\n",
    "                display(f'La data corretta è {data_probabile}')\n",
    "                df.at[i, colonna1_def] = data_probabile\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "        except:\n",
    "            print(f'La riga {i} da errore')\n",
    "    \n",
    "    \n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniforma_date_ship_date(df:pd.DataFrame, colonna_originale1:str, colonna_originale2:str)->pd.DataFrame:\n",
    "    '''\n",
    "Attempts to clean and standardize date values in a specified column (e.g., shipping dates),\n",
    "by converting them into a consistent datetime format (YYYY/MM/DD). It creates three new columns: one with \n",
    "the parsed datetime values from the original column, one for the reference string date column, and one containing \n",
    "the final corrected values.\n",
    "\n",
    "When the conversion of a date fails (resulting in a NaT), the function generates all possible permutations \n",
    "of the numeric parts of the raw date string. It then filters these permutations to keep only valid dates \n",
    "that occur after or on the date specified in a reference column (typically an order date). Among these, \n",
    "the function selects the one closest in time to the reference date.\n",
    "\n",
    "Parameters:\n",
    "- df: The input DataFrame.\n",
    "- colonna_originale1: The name of the column containing unclean or non-standard date strings (e.g., ship date).\n",
    "- colonna_originale2: The name of the reference column used to validate dates chronologically (e.g., order date).\n",
    "\n",
    "Returns:\n",
    "- df: The modified DataFrame, enriched with the following columns:\n",
    "    - '<colonna_originale1>_dt': parsed datetime version of colonna_originale1.\n",
    "    - '<colonna_originale2>_dt': parsed datetime version of colonna_originale2.\n",
    "    - '<colonna_originale1>_def': the cleaned and standardized date values for colonna_originale1.\n",
    "'''\n",
    "\n",
    "    #creo una colonna in formato datetime e una in formato stringa\n",
    "    #quella in formato datetime mi serve per pulire il grosso degli errori\n",
    "    #quella in formato stringa mi serve per pulire gli errori come per esempio se il mese supera il 12, o se è 0 ecc\n",
    "    #quindi quando la colonna dt mi da errore, lavoro sulla colonna str\n",
    "    # scelgo come formato unico il formato YYYY/MM/DD\n",
    "    colonna1_dt=colonna_originale1 + '_dt'\n",
    "    colonna2_dt=colonna_originale2 + '_dt'\n",
    "    colonna1_def=colonna_originale1 + '_def'\n",
    "\n",
    "\n",
    "    df[colonna1_dt] = pd.to_datetime(df[colonna_originale1], errors='coerce')\n",
    "    df[colonna2_dt] = pd.to_datetime(df[colonna_originale2], errors='coerce')\n",
    "    \n",
    "\n",
    "    #Inizializzo colonna finale\n",
    "    df[colonna1_def] = df[colonna1_dt]  # dove non è NaT, la data è già buona\n",
    "\n",
    "    \n",
    "    for i, row in df[pd.isna(df[colonna1_dt])].iterrows():\n",
    "        raw_date = str(row[colonna_originale1]) #rendo la data in stringa così posso lavorarci\n",
    "        try:\n",
    "            parts = re.split(r'\\D+', raw_date.strip()) #cosi faccio lo split non solo sullo / ma qualsiasi simbolo non numerico\n",
    "            combinazioni = ['/'.join(p) for p in permutations(parts)] #trovo tutte le combinazioni possibili\n",
    "            date_possibili = pd.to_datetime(combinazioni, errors='coerce') #rendo ogni combinazione una data in formato dt\n",
    "            date_valide = date_possibili[date_possibili.notna() & (date_possibili >= row[colonna2_dt])] #controllo che la data possibile sia successiva alla data di ordine\n",
    "            if not date_valide.empty:\n",
    "                distanze = date_valide - row[colonna2_dt]  # sarà sempre >= 0\n",
    "                indice_minimo = distanze.argmin()\n",
    "                data_probabile = date_valide.iloc[indice_minimo] #indice della differenza minima\n",
    "                df.at[i, colonna1_def] = data_probabile\n",
    "        except:\n",
    "            print(f'La riga {i} da errore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d71ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose2(dizionario):\n",
    "    '''\n",
    "    Prints the values from a dictionary that are different from their corresponding key.\n",
    "\n",
    "    Parameters:\n",
    "    dizionario (dict): A dictionary where keys represent the cleaned or standardized values,\n",
    "                       and values are lists of original values mapped to those keys.\n",
    "\n",
    "    Returns:\n",
    "    None: This function only prints output and does not return anything.\n",
    "    '''\n",
    "    for key, value_list in dizionario.items():\n",
    "        # Stampiamo solo se la lista ha più di un elemento\n",
    "        if len(value_list) > 1:\n",
    "            # Troviamo i valori diversi dalla chiave\n",
    "            diffs = [v for v in value_list if v != key]\n",
    "            if diffs:  # Se ce ne sono almeno uno\n",
    "                if len(diffs)==1:\n",
    "                    print(f\"Original value {diffs} is changed into {key}\")\n",
    "                else:\n",
    "                    print(f\"Original values {diffs} are changed into {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c2bf6",
   "metadata": {},
   "source": [
    "# GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Leggi tutti i fogli come dizionario di DataFrame\n",
    "xls = pd.ExcelFile(\"../data/raw/Superstore_mod_v1.xlsx\")\n",
    "df_dict = pd.read_excel(xls, sheet_name=None)  # None legge tutti i fogli\n",
    "\n",
    "# df_dict è un dizionario con chiavi = nomi dei fogli, valori = DataFrame\n",
    "for sheet_name, df in df_dict.items(): \n",
    "    globals()[f'df_{sheet_name.lower()}'] = df  # globals() in Python serve a restituire un dizionario contenente tutte le variabili globali \n",
    "                                                #attualmente disponibili nel contesto in cui viene chiamato\n",
    "if verbose==2:\n",
    "    print('Dataframe: countries')\n",
    "    display(df_countries)\n",
    "    print('Dataframe: orders')\n",
    "    display(df_orders)\n",
    "    print('Dataframe: customers')\n",
    "    display(df_customers)\n",
    "    print('Dataframe: products')\n",
    "    display(df_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c540a",
   "metadata": {},
   "source": [
    "# CLEANING DATA\n",
    "- for every column of every dataframe, a dictionary is created with each unique cleaned value and its original variants. This dictionary is converted into a dataframe too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92f25d",
   "metadata": {},
   "source": [
    "## COUNTRIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA STATE'''\n",
    "if verbose>=1:\n",
    "    print('When using the function `pulisci_stringhe_fuzz` with thresholds 90 and 80: NY and NC are not converted into NEW YORK and NORTH CAROLINA, so I corrected them manually.')\n",
    "    print('When using the function `pulisci_stringhe_lev` with threshold = 1: NC was incorrectly matched to NEW YORK, so I used a manual mapping to fix it to NORTH CAROLINA')\n",
    "    print(\"I choose the lev_1 one, as it is the one which returns the fewest errors.\")\n",
    "    print('') #per distaccare i print se verbose=1 dai print se verbose=2\n",
    "\n",
    "df_countries_raw=df_countries.drop_duplicates() #tolgo i duplicati da tutto il df_countries\n",
    "\n",
    "#FUNZIONE FUZZ SOGLIA 90\n",
    "df_countries_fuzz_90, dict_countries_fuzz_90= pulisci_stringhe_fuzz(df_countries_raw,'State',90) \n",
    "df_countries_fuzz_90 = df_countries_fuzz_90.rename(columns={'State_mod': 'State_fuzz_90'})\n",
    "df_countries_fuzz_90['State_fuzz_90'] = df_countries_fuzz_90['State_fuzz_90'].replace({\n",
    "    'NY': 'NEW YORK', \n",
    "    'NC': 'NORTH CAROLINA'\n",
    "})\n",
    "df_stati_corretti_fuzz_90=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_90.items() if len(v)>1], columns=['State', 'Variants_fuzz_90'])\n",
    "\n",
    "\n",
    "#FUNZIONE FUZZ SOGLIA 80\n",
    "df_countries_fuzz_80,dict_countries_fuzz_80= pulisci_stringhe_fuzz(df_countries_fuzz_90,'State',80)\n",
    "df_countries_fuzz_80= df_countries_fuzz_80.rename(columns={'State_mod':'State_fuzz_80'})\n",
    "df_countries_fuzz_80['State_fuzz_80']=df_countries_fuzz_80['State_fuzz_80'].replace({'NY': 'NEW YORK', 'NC': 'NORTH CAROLINA'}) \n",
    "df_stati_corretti_fuzz_80=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_80.items() if len(v)>1], columns=['State', 'Variants_fuzz_80'])\n",
    "\n",
    "\n",
    "\n",
    "#FUNZIONE LEV SOGLIA 1\n",
    "df_countries_lev_1, dict_countries_lev_1=pulisci_stringhe_lev(df_countries_fuzz_80,'State',1) #applico la funzione lev\n",
    "df_countries_lev_1= df_countries_lev_1.rename(columns={'State_mod':'State_lev_1'})\n",
    "df_countries_lev_1['State_lev_1']=df_countries_lev_1['State_lev_1'].replace({'NY': 'NEW YORK'}) \n",
    "#df_countries_lev_1, guardando gli output, è quello che si avvicina di più ad essere corretto\n",
    "state_map = {\n",
    "    'NC': 'NORTH CAROLINA'\n",
    "}\n",
    "df_countries_lev_1['State_lev_1'] = df_countries_lev_1['State'].map(state_map).fillna(df_countries_lev_1['State_lev_1'])\n",
    "df_stati_corretti_lev_1=pd.DataFrame([(k, v) for k, v in dict_countries_lev_1.items() if len(v) >1], columns=['State', 'Variants_lev_1'])\n",
    "\n",
    "#MERGE TRA I 3 DF\n",
    "df_merge_state_lev_fuzz90 = df_stati_corretti_fuzz_90.merge(df_stati_corretti_lev_1, on = 'State', how = 'outer')\n",
    "df_merge_state_lev_fuzz90_fuzz80= df_merge_state_lev_fuzz90.merge(df_stati_corretti_fuzz_80, on='State', how='outer' )\n",
    "\n",
    "\n",
    "if verbose ==2:\n",
    "    print(\"Using fuzz function with threshold=90:\")\n",
    "    verbose2(dict_countries_fuzz_90)\n",
    "    print('Using replace, i change NY in NEW YORK and NC in NORTH CAROLINA')\n",
    "    print('')\n",
    "\n",
    "    print(\"Using fuzz function with threshold=80:\")\n",
    "    verbose2(dict_countries_fuzz_80)\n",
    "    print('Using replace, i change NY in NEW YORK and NC in NORTH CAROLINA')\n",
    "    print('')\n",
    "\n",
    "    print(\"Using levenshtein function with threshold=1:\")\n",
    "    verbose2(dict_countries_lev_1)\n",
    "    print('Since NC is changed into NY, I use a map to fix it, and then i use .replace in order to change it in NEW YORK')\n",
    "    print('')\n",
    "\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_merge_state_lev_fuzz90_fuzz80)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e2eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'pulisci_stringhe_fuzz' with a threshold of 95, the cities 'BOWLINGGREEN', 'WOODSTOK', and 'ENDERSON' are not automatically corrected to 'BOWLING GREEN', 'WOODSTOCK', and 'HENDERSON'.\n",
      "These cases are manually corrected\n",
      "Among all approaches, the function with fuzziness threshold = 95 produces the best results\n",
      "as the others cause too many incorrect city matches.\n",
      "\n",
      "Using fuzz function with threshold=90: \n",
      "Original value ['BOWLING GREEN'] is changed into BOWLINGGREEN\n",
      "Original value ['AMARILLO'] is changed into CAMARILLO\n",
      "Original value ['HENDERSON'] is changed into ENDERSON\n",
      "Original value ['OVERLAND PARK'] is changed into ORLAND PARK\n",
      "Original value ['EDMOND'] is changed into REDMOND\n",
      "Original value ['TRENTON'] is changed into RENTON\n",
      "Original value ['WOODSTOCK'] is changed into WOODSTOK\n",
      "\n",
      "Using fuzz function with threshold=95:\n",
      "Original value ['BOWLING GREEN'] is changed into BOWLINGGREEN\n",
      "I choose this function because is the one with least errors\n",
      "\n",
      "Using levenshtein function with threshold=1:\n",
      "Original value ['MEDFORD'] is changed into BEDFORD\n",
      "Original value ['BOWLING GREEN'] is changed into BOWLINGGREEN\n",
      "Original value ['AMARILLO'] is changed into CAMARILLO\n",
      "Original value ['CLINTON'] is changed into CLIFTON\n",
      "Original value ['HENDERSON'] is changed into ENDERSON\n",
      "Original value ['INGLEWOOD'] is changed into ENGLEWOOD\n",
      "Original value ['LAWTON'] is changed into LAYTON\n",
      "Original value ['MASON'] is changed into MACON\n",
      "Original value ['CONROE'] is changed into MONROE\n",
      "Original value ['NORMAN'] is changed into NORMAL\n",
      "Original value ['REDDING'] is changed into READING\n",
      "Original value ['EDMOND'] is changed into REDMOND\n",
      "Original value ['TRENTON'] is changed into RENTON\n",
      "Original value ['WOODSTOCK'] is changed into WOODSTOK\n",
      "\n",
      "The 'cleaning' dataframe is:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Variants_lev_1</th>\n",
       "      <th>Variants_fuzz_95</th>\n",
       "      <th>Variants_fuzz_90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>[BEDFORD, MEDFORD]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOWLINGGREEN</td>\n",
       "      <td>[BOWLING GREEN, BOWLINGGREEN]</td>\n",
       "      <td>[BOWLING GREEN, BOWLINGGREEN]</td>\n",
       "      <td>[BOWLING GREEN, BOWLINGGREEN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAMARILLO</td>\n",
       "      <td>[AMARILLO, CAMARILLO]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[AMARILLO, CAMARILLO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLIFTON</td>\n",
       "      <td>[CLIFTON, CLINTON]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENDERSON</td>\n",
       "      <td>[ENDERSON, HENDERSON]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ENDERSON, HENDERSON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENGLEWOOD</td>\n",
       "      <td>[ENGLEWOOD, INGLEWOOD]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LAYTON</td>\n",
       "      <td>[LAWTON, LAYTON]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MACON</td>\n",
       "      <td>[MACON, MASON]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MONROE</td>\n",
       "      <td>[CONROE, MONROE]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NORMAL</td>\n",
       "      <td>[NORMAL, NORMAN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ORLAND PARK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ORLAND PARK, OVERLAND PARK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>READING</td>\n",
       "      <td>[READING, REDDING]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>REDMOND</td>\n",
       "      <td>[EDMOND, REDMOND]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[EDMOND, REDMOND]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RENTON</td>\n",
       "      <td>[RENTON, TRENTON]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[RENTON, TRENTON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WOODSTOK</td>\n",
       "      <td>[WOODSTOCK, WOODSTOK]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[WOODSTOCK, WOODSTOK]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City                 Variants_lev_1  \\\n",
       "0        BEDFORD             [BEDFORD, MEDFORD]   \n",
       "1   BOWLINGGREEN  [BOWLING GREEN, BOWLINGGREEN]   \n",
       "2      CAMARILLO          [AMARILLO, CAMARILLO]   \n",
       "3        CLIFTON             [CLIFTON, CLINTON]   \n",
       "4       ENDERSON          [ENDERSON, HENDERSON]   \n",
       "5      ENGLEWOOD         [ENGLEWOOD, INGLEWOOD]   \n",
       "6         LAYTON               [LAWTON, LAYTON]   \n",
       "7          MACON                 [MACON, MASON]   \n",
       "8         MONROE               [CONROE, MONROE]   \n",
       "9         NORMAL               [NORMAL, NORMAN]   \n",
       "10   ORLAND PARK                            NaN   \n",
       "11       READING             [READING, REDDING]   \n",
       "12       REDMOND              [EDMOND, REDMOND]   \n",
       "13        RENTON              [RENTON, TRENTON]   \n",
       "14      WOODSTOK          [WOODSTOCK, WOODSTOK]   \n",
       "\n",
       "                 Variants_fuzz_95               Variants_fuzz_90  \n",
       "0                             NaN                            NaN  \n",
       "1   [BOWLING GREEN, BOWLINGGREEN]  [BOWLING GREEN, BOWLINGGREEN]  \n",
       "2                             NaN          [AMARILLO, CAMARILLO]  \n",
       "3                             NaN                            NaN  \n",
       "4                             NaN          [ENDERSON, HENDERSON]  \n",
       "5                             NaN                            NaN  \n",
       "6                             NaN                            NaN  \n",
       "7                             NaN                            NaN  \n",
       "8                             NaN                            NaN  \n",
       "9                             NaN                            NaN  \n",
       "10                            NaN   [ORLAND PARK, OVERLAND PARK]  \n",
       "11                            NaN                            NaN  \n",
       "12                            NaN              [EDMOND, REDMOND]  \n",
       "13                            NaN              [RENTON, TRENTON]  \n",
       "14                            NaN          [WOODSTOCK, WOODSTOK]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''COLONNA CITY'''\n",
    "if verbose >=1:\n",
    "    print(\"Using 'pulisci_stringhe_fuzz' with a threshold of 95, the cities 'BOWLINGGREEN', 'WOODSTOK', and 'ENDERSON' are not automatically corrected to 'BOWLING GREEN', 'WOODSTOCK', and 'HENDERSON'.\")\n",
    "    print(\"These cases are manually corrected\")\n",
    "    print(\"Among all approaches, the function with fuzziness threshold = 95 produces the best results\")\n",
    "    print(\"as the others cause too many incorrect city matches.\")\n",
    "    print('')\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 90\n",
    "df_countries_fuzz_city_90, dict_countries_fuzz_city_90= pulisci_stringhe_fuzz(df_countries_lev_1,'City',90) \n",
    "df_countries_fuzz_city_90 = df_countries_fuzz_city_90.rename(columns={'City_mod': 'City_fuzz_90'})\n",
    "df_countries_city_corrette_fuzz_90=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_90.items() if len(v)>1 ], columns=['City', 'Variants_fuzz_90'])\n",
    "\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 95\n",
    "df_countries_fuzz_city_95,dict_countries_fuzz_city_95= pulisci_stringhe_fuzz(df_countries_fuzz_city_90,'City',95) \n",
    "df_countries_fuzz_city_95 = df_countries_fuzz_city_95.rename(columns={'City_mod': 'City_fuzz_95'})\n",
    "df_countries_fuzz_city_95['City_fuzz_95']= df_countries_fuzz_city_95['City_fuzz_95'].replace({'BOWLINGGREEN': 'BOWLING GREEN'}).replace({'WOODSTOK': 'WOODSTOCK'})\n",
    "df_countries_city_corrette_fuzz_95=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_95.items() if len(v)>1 ], columns=['City', 'Variants_fuzz_95'])\n",
    "df_countries_fuzz_city_95 = df_countries_fuzz_city_95.rename(columns={'City_fuzz_95': 'City_def'})\n",
    "df_countries_fuzz_city_95['City_def']=df_countries_fuzz_city_95['City_def'].replace({\n",
    "    'ENDERSON':'HENDERSON'\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "#FUNZIONE LEV CON SOGLIA 1\n",
    "df_countries_lev_city_1, dict_countries_lev_city_1= pulisci_stringhe_lev(df_countries_fuzz_city_95,'City',1) \n",
    "df_countries_lev_city_1 = df_countries_lev_city_1.rename(columns={'City_mod': 'City_lev_1'})\n",
    "df_countries_city_corrette_lev_1=pd.DataFrame([(k, v) for k, v in dict_countries_lev_city_1.items() if len(v)>1], columns=['City', 'Variants_lev_1'])\n",
    "\n",
    "\n",
    "#MERGE TRA I 3 DF\n",
    "df_merge_city_fuzz_95_fuzz_90=df_countries_city_corrette_fuzz_95.merge(df_countries_city_corrette_fuzz_90, on='City', how='outer')\n",
    "df_merge_city_fuzz_95_fuzz_90_lev_1=df_countries_city_corrette_lev_1.merge(df_merge_city_fuzz_95_fuzz_90, on='City', how='outer')\n",
    "\n",
    "\n",
    "#METODO PIù GIUSTO: FUZZ CON SOGLIA=95 ci sono pochi errori in questa colonna city, ce ne sono molti nella colonna city del foglio orders, ancora da finire\n",
    "df_countries_lev_city_1=df_countries_lev_city_1[['Country','City','City_fuzz_90','City_def','City_lev_1','State','State_fuzz_90','State_fuzz_80','State_lev_1','Postal Code','Region']]\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"Using fuzz function with threshold=90: \")\n",
    "    verbose2(dict_countries_fuzz_city_90)\n",
    "    print('')\n",
    "\n",
    "    print(\"Using fuzz function with threshold=95:\")\n",
    "    verbose2(dict_countries_fuzz_city_95)\n",
    "    print('I choose this function because is the one with least errors')\n",
    "    print('')\n",
    "\n",
    "    print(\"Using levenshtein function with threshold=1:\")\n",
    "    verbose2(dict_countries_lev_city_1)\n",
    "    print('')\n",
    "\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_merge_city_fuzz_95_fuzz_90_lev_1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a169070",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ANCORA COLONNA CITY MA CON CONTROLLO DELLO STATO PER MODIFICARE LA CITTA: MODIFICO LA CITTA' SOLO SE HANNO LO STATO UGUALE'''\n",
    "\n",
    "if verbose>=1:\n",
    "    print(\"In this section, I apply string-cleaning functions that modify city names only if the corresponding state (column 'State_lev_1') is the same.\")\n",
    "    print(\"This helps avoid incorrect matches between cities with the same or similar names but located in different states.\")\n",
    "    print(\"I use the modified versions of the functions (pulisci_stringhe_fuzz2 and pulisci_stringhe_lev2), which include an additional state-level condition.\")\n",
    "\n",
    "#METODO FUZZ (2) CON SOGLIA 90\n",
    "df_countries_fuzz_city_90_c, dict_countries_fuzz_city_90_c= pulisci_stringhe_fuzz2(df_countries_lev_1,'City',90,'State_lev_1') \n",
    "df_countries_fuzz_city_90_c = df_countries_fuzz_city_90_c.rename(columns={'City_mod': 'City_fuzz_90'})\n",
    "df_countries_city_corrette_fuzz_90_c=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_90_c.items() if len(v)>1], columns=['City', 'Variants_fuzz_90'])\n",
    "\n",
    "\n",
    "#METODO FUZZ (2) CON SOGLIA 95\n",
    "df_countries_fuzz_city_95_c,dict_countries_fuzz_city_95_c= pulisci_stringhe_fuzz2(df_countries_fuzz_city_90_c,'City',95,'State_lev_1') \n",
    "df_countries_fuzz_city_95_c = df_countries_fuzz_city_95_c.rename(columns={'City_mod': 'City_fuzz_95'})\n",
    "df_countries_city_corrette_fuzz_95_c=pd.DataFrame([(k, v) for k, v in dict_countries_fuzz_city_95_c.items() if len(v)>1], columns=['City', 'Variants_fuzz_95'])\n",
    "\n",
    "\n",
    "\n",
    "#METODO LEV (2) CON SOGLIA 1\n",
    "df_countries_lev_city_1_c, dict_countries_lev_city_1_c= pulisci_stringhe_lev2(df_countries_fuzz_city_95_c,'City',1,'State_lev_1') \n",
    "df_countries_lev_city_1_c = df_countries_lev_city_1_c.rename(columns={'City_mod': 'City_lev_1'})\n",
    "df_countries_city_corrette_lev_1_c=pd.DataFrame([(k, v) for k, v in dict_countries_lev_city_1_c.items() if len(v)>1], columns=['City', 'Variants_lev_1'])\n",
    "\n",
    "\n",
    "#MERGE TRA I 3 DF\n",
    "df_merge_city_fuzz_95_fuzz_90_c=df_countries_city_corrette_fuzz_95_c.merge(df_countries_city_corrette_fuzz_90_c, on='City', how='outer')\n",
    "df_merge_city_fuzz_95_fuzz_90_lev_1_c=df_countries_city_corrette_lev_1_c.merge(df_merge_city_fuzz_95_fuzz_90_c, on='City', how='outer')\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"Using fuzz function with threshold=90, checking the state as well, the 'cleaning' dataframe is:\")\n",
    "    verbose2(dict_countries_fuzz_city_90_c)\n",
    "    print('')\n",
    "\n",
    "    print(\"Using fuzz function with threshold=95, checking the state as well, the 'cleaning' dataframe is:\")\n",
    "    verbose2(dict_countries_fuzz_city_95_c)\n",
    "    print('')\n",
    "\n",
    "    print(\"Using levenshtein function with threshold=1, checking the state as well, the 'cleaning' dataframe is:\")\n",
    "    verbose2(dict_countries_lev_city_1_c)\n",
    "    print('')\n",
    "\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_merge_city_fuzz_95_fuzz_90_lev_1_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be0b1e",
   "metadata": {},
   "source": [
    "## ORDERS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_raw=df_orders\n",
    "df_orders_mod=df_orders_raw.copy().drop_duplicates()\n",
    "\n",
    "'''COLONNA SHIP MODE'''  \n",
    "#METODO LEV CON SOGLIA 5\n",
    "df_orders_ship_mode_lev_1, dict_orders_ship_mode_lev_1 =pulisci_stringhe_lev(df_orders_mod,'Ship Mode',5)\n",
    "df_ship_mode_mod_corrette=pd.DataFrame([(k, v) for k, v in dict_orders_ship_mode_lev_1.items() if len(v)>1], columns=['Ship_mode', 'Variants'])\n",
    "\n",
    "mask_2_class = df_orders_ship_mode_lev_1['Ship Mode'] == '2 Class'\n",
    "df_orders_ship_mode_lev_1.loc[mask_2_class, 'Ship Mode_mod'] = 'SECOND CLASS'\n",
    "\n",
    "#display(df_orders_ship_mode_lev_1.loc[df_orders_ship_mode_lev_1['Ship Mode']=='2 Class'])\n",
    "\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"Using levenshtein function with threshold=5, the 'cleaning' dataframe is:\")\n",
    "    display(df_ship_mode_mod_corrette)\n",
    "    verbose2(dict_orders_ship_mode_lev_1)\n",
    "    print(\"Since it mistakenly change '2 CLASS' in 'FIRST CLASS', I manually fix it in the dataframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1dfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA SALES'''\n",
    "if verbose>=1:\n",
    "    print(\" The `pulisci_trattino` function is applied to clean the 'Sales' column, replacing dashes (`-`) that are not at the beginning of the string.\")\n",
    "    print(\"Then, additional cleaning is done replacing commas (`,`) and apostrophes (`'`) with dots (`.`) and to remove any unintended characters like `a`.\")\n",
    "\n",
    "df_orders_ship_mode_lev_1['Sales_mod']= df_orders_ship_mode_lev_1['Sales'].apply(pulisci_trattino) # sostitiusco il - con il . quando è in mezzo alla stringa\n",
    "df_orders_ship_mode_lev_1['Sales_mod'] = df_orders_ship_mode_lev_1['Sales_mod'].astype(str).str.strip() # rimuovo eventuali spazi bianchi e converto in string\n",
    "df_orders_ship_mode_lev_1['Sales_mod'] = df_orders_ship_mode_lev_1['Sales_mod'].str.replace(\",\", '.', regex=False).str.replace(\"'\", '.', regex=False).str.replace('a', '', regex=False) # sostituisco le virgole con punti (per uniformare la notazione decimale), il ' e la a\n",
    "\n",
    "df_orders_ship_mode_lev_1['Sales_mod'] = pd.to_numeric(df_orders_ship_mode_lev_1['Sales_mod'], errors='coerce')\n",
    "dict_sales_mod = creo_dict2(df_orders_ship_mode_lev_1,'Sales','Sales_mod')\n",
    "df_sales_mod_corrette=pd.DataFrame([(k, v) for k, v in dict_sales_mod.items() if len(v)>1], columns=['Sales', 'Variants'])\n",
    "\n",
    "\n",
    "df_orders_ship_mode_lev_1=df_orders_ship_mode_lev_1[['Order ID','Order Date','Ship Date','Ship Mode','Ship Mode_mod','Customer ID','City','Postal Code','Product ID', 'Sales','Sales_mod','Quantity','Discount','Profit']] #avvicino Sales_mod a Sales\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_sales_mod_corrette)\n",
    "    verbose2(dict_sales_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA QUANTITY'''\n",
    "if verbose>=1:\n",
    "    print(\"The `pulisci_trattino` function is applied to clean the 'Quantity' column, replacing dashes (`-`) that are not at the beginning of the string.\")\n",
    "    print(\"Then, additional cleaning is done replacing commas (`,`) and apostrophes (`'`) with dots (`.`) and to remove any unintended characters like `ì`.\")\n",
    "\n",
    "    \n",
    "df_orders_ship_mode_lev_1['Quantity_mod']=df_orders_ship_mode_lev_1['Quantity'].astype(str).apply(pulisci_trattino)\n",
    "df_orders_ship_mode_lev_1['Quantity_mod'] = df_orders_ship_mode_lev_1['Quantity_mod'].str.replace(\",\", '.', regex=False).str.replace(\"'\", '.', regex=False).str.replace('ì', '', regex=False) # sostituisco le virgole con punti (per uniformare la notazione decimale), l'apostrofo ' e la ì\n",
    "\n",
    "df_orders_ship_mode_lev_1['Quantity_mod'] = pd.to_numeric(df_orders_ship_mode_lev_1['Quantity_mod'], errors='coerce')\n",
    "dict_quantity_mod=creo_dict2(df_orders_ship_mode_lev_1,'Quantity','Quantity_mod')\n",
    "df_quantity_mod_corrette=pd.DataFrame([(k, v) for k, v in dict_quantity_mod.items() if len(v)>1], columns=['Quantity', 'Variants'])\n",
    "\n",
    "if verbose==2:\n",
    "    print('The cleaning dataframe is:')\n",
    "    display(df_quantity_mod_corrette)\n",
    "    verbose2(dict_quantity_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d214d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA DISCOUNT'''\n",
    "if verbose>=1:\n",
    "    print(\"Cleaning is done using '.replace()' to remove any unintended characters like 'ù' and '%'.\")\n",
    "df_orders_ship_mode_lev_1['Discount_mod'] = df_orders_ship_mode_lev_1['Discount'].astype(str).str.replace(\"ù\", '', regex=False).str.replace(\"%\", '', regex=False)\n",
    "\n",
    "df_orders_ship_mode_lev_1['Discount_mod']=pd.to_numeric(df_orders_ship_mode_lev_1['Discount_mod'],errors='coerce')\n",
    "dict_discount_mod=creo_dict(df_orders_ship_mode_lev_1,'Discount','Discount_mod')\n",
    "df_discount_mod_corretto=pd.DataFrame([(k, v) for k, v in dict_discount_mod.items() if len(v)>1], columns=['Discount', 'Variants'])\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_discount_mod_corretto)\n",
    "    verbose2(dict_discount_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA PROFIT'''\n",
    "if verbose>=1:\n",
    "    print(\"The `pulisci_trattino` function is applied to clean the 'Profit' column, replacing dashes (`-`) that are **not at the beginning** of the string.\")\n",
    "    print(\"Then, additional cleaning is done using `.replace()` to: replace commas (`,`) and apostrophes (`'`) with dots (`.`) and to remove any unintended characters like `a`.\")\n",
    "\n",
    "df_orders_ship_mode_lev_1['Profit_mod']= df_orders_ship_mode_lev_1['Profit'].astype(str).apply(pulisci_trattino)\n",
    "df_orders_ship_mode_lev_1['Profit_mod'] = df_orders_ship_mode_lev_1['Profit_mod'].str.strip() # 1. Rimuovi eventuali spazi bianchi e converti in string\n",
    "df_orders_ship_mode_lev_1['Profit_mod'] = df_orders_ship_mode_lev_1['Profit_mod'].str.replace(\",\", '.', regex=False).str.replace(\"'\", '.', regex=False).str.replace('a', '', regex=False) # 2. Sostituisci le virgole con punti (per uniformare la notazione decimale)\n",
    "\n",
    "df_orders_ship_mode_lev_1['Profit_mod'] = pd.to_numeric(df_orders_ship_mode_lev_1['Profit_mod'], errors='coerce')\n",
    "dict_profit_mod=creo_dict2(df_orders_ship_mode_lev_1,'Profit','Profit_mod')\n",
    "df_profit_mod_corretto=pd.DataFrame([(k, v) for k, v in dict_profit_mod.items() if len(v)>1 ], columns=['Profit', 'Variants'])\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_profit_mod_corretto)\n",
    "    verbose2(dict_profit_mod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75676be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA CITY'''\n",
    "\n",
    "if verbose >= 1:\n",
    "    print(\"Cleaning the 'City' column using string similarity functions (fuzz and levenshtein) with different thresholds.\")\n",
    "    print(\"- First, I apply 'pulisci_stringhe_fuzz' with thresholds 90 and 85.\")\n",
    "    print(\"- Then, I use 'pulisci_stringhe_lev' with threshold=1, which gives the best results with the least errors.\")\n",
    "    print(\"- Despite that, some cities are still mistakenly changed or left unchanged:\")\n",
    "    print(\"  - I manually restore wrongly modified cities using a mask.\")\n",
    "    print(\"  - I manually correct specific city names not automatically fixed.\")\n",
    "    print(\"- Finally, I also test 'pulisci_stringhe_lev' with threshold=2, but it introduces too many false corrections.\")\n",
    "\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 90\n",
    "df_orders_fuzz_city_90, dict_orders_city_fuzz_90= pulisci_stringhe_fuzz(df_orders_ship_mode_lev_1,'City',90) \n",
    "df_orders_fuzz_city_90 = df_orders_fuzz_city_90.rename(columns={'City_mod': 'City_fuzz_90'})\n",
    "df_orders_fuzz_city_90['City_fuzz_90']= df_orders_fuzz_city_90['City_fuzz_90'].replace({'BOWLINGGREEN': 'BOWLING GREEN'})\n",
    "df_orders_city_corrette_fuzz_90=pd.DataFrame([(k, v) for k, v in dict_orders_city_fuzz_90.items() if len(v)>1], columns=['City', 'Variants_fuzz_90'])\n",
    "\n",
    "#FUNZIONE FUZZ CON SOGLIA 85\n",
    "df_orders_fuzz_city_85,dict_orders_city_fuzz_85=pulisci_stringhe_fuzz(df_orders_fuzz_city_90,'City',85) \n",
    "df_orders_fuzz_city_85 = df_orders_fuzz_city_85.rename(columns={'City_mod': 'City_fuzz_85'})\n",
    "df_orders_city_corrette_fuzz_85=pd.DataFrame([(k, v) for k, v in dict_orders_city_fuzz_85.items() if len(v)>1], columns=['City', 'Variants_fuzz_85'])\n",
    "\n",
    "\n",
    "\n",
    "#FUNZIONE LEV CON SOGLIA 1\n",
    "df_orders_lev_city_1,dict_orders_city_lev_1=pulisci_stringhe_lev(df_orders_fuzz_city_85,'City',1) # quello con meno errori\n",
    "df_orders_lev_city_1 = df_orders_lev_city_1.rename(columns={'City_mod': 'City_lev_1'})\n",
    "citta_da_ripristinare = ['Conroe', 'Redding', 'Camarillo', 'Englewood', 'Mason', 'Medford', 'Normal', 'Layton', 'Renton','Clifton','Edmond'] #SISTEMO LE CITTà CHE HA RIMOSSO MA NON DOVEVA\n",
    "mask = df_orders_lev_city_1['City'].isin(citta_da_ripristinare)\n",
    "df_orders_lev_city_1.loc[mask, 'City_lev_1'] = df_orders_lev_city_1.loc[mask, 'City'].str.upper()\n",
    "df_orders_city_corrette_lev_1=pd.DataFrame([(k, v) for k, v in dict_orders_city_lev_1.items() if len(v)>1], columns=['City', 'Variants_lev_1'])\n",
    "df_orders_lev_city_1['City_lev_1']=df_orders_lev_city_1['City_lev_1'].replace({\n",
    "    'ALECSANDRIA':'ALEXANDRIA',\n",
    "    'LA':'LOS ANGELES',\n",
    "    'MARLBOROUG':'MARLBOROUGH',\n",
    "    'FILADELPHIA':'PHILADELPHIA',\n",
    "    'NY CITY':'NEW YORK CITY',\n",
    "    'NYC':'NEW YORK CITY',\n",
    "    'EDINBURG':'EDINBURGH'\n",
    "})\n",
    "df_orders_lev_city_1=df_orders_lev_city_1.rename(columns={'City_lev_1':'City_def'})\n",
    "\n",
    "\n",
    "#FUNZIONE LEV CON SOGLIA 2\n",
    "df_orders_lev_city_2,dict_orders_city_lev_2=pulisci_stringhe_lev(df_orders_lev_city_1,'City',2) #non va bene, rimuove troppe città corrette\n",
    "df_orders_lev_city_2 = df_orders_lev_city_2.rename(columns={'City_mod': 'City_lev_2'})\n",
    "df_orders_city_corrette_lev_2=pd.DataFrame([(k, v) for k, v in dict_orders_city_lev_2.items() if len(v)>1], columns=['City', 'Variants_lev_2'])\n",
    "\n",
    "\n",
    "\n",
    "#FACCIO IL MERGE TRA I 4 DF\n",
    "df_merge_orders_city_fuzz_90_fuzz_85=df_orders_city_corrette_fuzz_90.merge(df_orders_city_corrette_fuzz_85, on='City',how='outer')\n",
    "df_merge_orders_city_fuzz_90_fuzz_85_lev_1=df_orders_city_corrette_lev_1.merge(df_merge_orders_city_fuzz_90_fuzz_85, on='City', how='outer')\n",
    "df_merge_orders_city_fuzz_90_fuzz_85_lev_1_lev_2=df_orders_city_corrette_lev_2.merge(df_merge_orders_city_fuzz_90_fuzz_85_lev_1, on='City', how='outer')\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_merge_orders_city_fuzz_90_fuzz_85_lev_1_lev_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66990a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the 'Order Date' column using the 'uniforma_date_order_date' function.\n",
      "Some dates cannot be parsed correctly using standard permutations (e.g., YYYY-MM-DD).\n",
      "For these specific cases (e.g., '13/13/2014'), I manually correct the values.\n",
      "Then, I drop temporary columns \n",
      "The 'cleaning' dataframe is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/92/7kxqqyld1rl76prg4ll1c3s40000gn/T/ipykernel_87334/40077103.py:41: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_possibili = pd.to_datetime(combinazioni, errors='coerce') #rendo ogni combinazione una data in formato dt\n",
      "/var/folders/92/7kxqqyld1rl76prg4ll1c3s40000gn/T/ipykernel_87334/40077103.py:41: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_possibili = pd.to_datetime(combinazioni, errors='coerce') #rendo ogni combinazione una data in formato dt\n",
      "/var/folders/92/7kxqqyld1rl76prg4ll1c3s40000gn/T/ipykernel_87334/40077103.py:41: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_possibili = pd.to_datetime(combinazioni, errors='coerce') #rendo ogni combinazione una data in formato dt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Variants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>[2011-12-31 00:00:00, 12/31/2011]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-17</td>\n",
       "      <td>[17/Jan/2012, 2012-01-17 00:00:00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-18</td>\n",
       "      <td>[18/9/2013, 2013-09-18 00:00:00]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-03-05</td>\n",
       "      <td>[2014-03-05 00:00:00, 5th March 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-04-07</td>\n",
       "      <td>[2014-04-07 00:00:00, 04/07/14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014-08-08</td>\n",
       "      <td>[2014-08-08 00:00:00, 8/08/2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014-09-29</td>\n",
       "      <td>[2014-09-29 00:00:00, 29-09-2014]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014-11-09</td>\n",
       "      <td>[2014-11-09 00:00:00, 11/9/2014]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order Date                             Variants\n",
       "0 2011-12-31    [2011-12-31 00:00:00, 12/31/2011]\n",
       "1 2012-01-17   [17/Jan/2012, 2012-01-17 00:00:00]\n",
       "2 2013-09-18     [18/9/2013, 2013-09-18 00:00:00]\n",
       "3 2014-03-05  [2014-03-05 00:00:00, 5th March 14]\n",
       "4 2014-04-07      [2014-04-07 00:00:00, 04/07/14]\n",
       "5 2014-08-08     [2014-08-08 00:00:00, 8/08/2014]\n",
       "6 2014-09-29    [2014-09-29 00:00:00, 29-09-2014]\n",
       "7 2014-11-09     [2014-11-09 00:00:00, 11/9/2014]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value ['12/31/2011'] is changed into 2011-12-31 00:00:00\n",
      "Original value ['17/Jan/2012'] is changed into 2012-01-17 00:00:00\n",
      "Original value ['18/9/2013'] is changed into 2013-09-18 00:00:00\n",
      "Original value ['5th March 14'] is changed into 2014-03-05 00:00:00\n",
      "Original value ['04/07/14'] is changed into 2014-04-07 00:00:00\n",
      "Original value ['8/08/2014'] is changed into 2014-08-08 00:00:00\n",
      "Original value ['29-09-2014'] is changed into 2014-09-29 00:00:00\n",
      "Original value ['11/9/2014'] is changed into 2014-11-09 00:00:00\n"
     ]
    }
   ],
   "source": [
    "'''COLONNA ORDER DATE'''\n",
    "if verbose>=1:\n",
    "    print(\"Cleaning the 'Order Date' column using the 'uniforma_date_order_date' function.\")\n",
    "    print(\"Some dates cannot be parsed correctly using standard permutations (e.g., YYYY-MM-DD).\")\n",
    "    print(\"For these specific cases (e.g., '13/13/2014'), I manually correct the values.\")\n",
    "    print(\"Then, I drop temporary columns \")\n",
    "\n",
    "\n",
    "df_date_pulite=uniforma_date_order_date(df_orders_lev_city_1,'Order Date','Ship Date')\n",
    "display(df_date_pulite)\n",
    "\n",
    "dict_order_date=creo_dict(df_date_pulite,'Order Date','Order Date_def')\n",
    "\n",
    "df_order_date_corrette=pd.DataFrame([(k, v) for k, v in dict_order_date.items() if len(v)>1], columns=['Order Date', 'Variants'])\n",
    "\n",
    "#C'è ANCORA IL PROBLEMA DELLE RIGHE COME RIGA (422 OPPURE 513) CON LA DATA 13/13/2014 CHE ANCHE SE GUARDO LE DIVERSE COMBINAZIONI, NON HA NESSUNA DATA VALIDA, PER QUESTE DUE USO IL REPLACE\n",
    "df_date_pulite.loc[df_date_pulite['Order Date'] == '13/13/2014', ['Order Date_dt', 'Order Date_def']] = pd.Timestamp('2014-10-13')\n",
    "df_date_pulite.loc[df_date_pulite['Order Date'] == '25/13/2011', ['Order Date_dt', 'Order Date_def']] = pd.Timestamp('2011-11-25')\n",
    "df_date_pulite.loc[df_date_pulite['Order Date'] == '14/03/013', ['Order Date_dt', 'Order Date_def']] = pd.Timestamp('2013-03-14')\n",
    "\n",
    "df_date_pulite=df_date_pulite.drop(columns=['Order Date_dt','Ship Date_dt'])\n",
    "\n",
    "#if verbose==2:\n",
    "    #print(\"The 'cleaning' dataframe is:\")\n",
    "    #display(df_order_date_corrette)\n",
    "    #verbose2(dict_order_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA SHIP DATE'''\n",
    "if verbose>=1:\n",
    "    print(\"Cleaning the 'Ship Date' column using the 'uniforma_date_ship_date' function.\")\n",
    "    print(\"Some dates cannot be parsed correctly using standard permutations (e.g., YYYY-MM-DD).\")\n",
    "    print(\"For these specific cases (e.g., '22/0/2013'), I manually correct the values.\")\n",
    "    print(\"Then, I drop temporary columns \")\n",
    "\n",
    "df_date_pulite2=uniforma_date_ship_date(df_date_pulite,'Ship Date', 'Order Date')\n",
    "dict_ship_date=creo_dict(df_date_pulite2,'Ship Date','Ship Date_def')\n",
    "df_ship_date_corrette=pd.DataFrame([(k, v) for k, v in dict_ship_date.items() if len(v)>1], columns=['Ship Date', 'Variants'])\n",
    "\n",
    "\n",
    "#devo pulire ancora le date in cui nessuna combniazione mi da una data possibile oppure altri errori\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '02 Sép 2013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-09-02')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '22/0/2013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-10-22')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '28/13/2013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-05-28')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '15/09/013', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2013-09-15')\n",
    "df_date_pulite2.loc[df_date_pulite2['Ship Date'] == '10_12_2012', ['Ship Date_dt', 'Ship Date_def']] = pd.Timestamp('2012-12-10')\n",
    "df_date_pulite2=df_date_pulite2.drop(columns=['Ship Date_dt','Order Date_dt'])\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"The 'cleaning' dataframe is:\")\n",
    "    display(df_ship_date_corrette)\n",
    "    verbose2(dict_ship_date)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c1bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA PRODUCT ID'''\n",
    "if verbose >= 1:\n",
    "    print(\"Cleaning the 'Product ID' column by removing unwanted characters like 'ù'.\")\n",
    "    print(\"Afterwards, the string is reformatted into the standard structure 'AAA-BB-1234567'.\")\n",
    "\n",
    "df_date_pulite2['Product ID_def']=df_date_pulite2['Product ID'].copy()\n",
    "\n",
    "df_date_pulite2['Product ID_def'] = (\n",
    "    df_date_pulite2['Product ID_def']\n",
    "    .str.replace('ù', '', regex=False)\n",
    "    .str.upper()\n",
    "    .str.replace(' ', '', regex=False)\n",
    "    .str.replace('-', '', regex=False)\n",
    ")\n",
    "\n",
    "\n",
    "#Applico il formato AAA-BB-1234567\n",
    "df_date_pulite2['Product ID_def'] = (\n",
    "    df_date_pulite2['Product ID_def'].str[:3] + '-' +\n",
    "    df_date_pulite2['Product ID_def'].str[3:5] + '-' +\n",
    "    df_date_pulite2['Product ID_def'].str[5:]\n",
    ")\n",
    "if verbose==2:\n",
    "    print('The dataframe with reformatted Product ID column is:')\n",
    "    display(df_date_pulite2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04aaea1",
   "metadata": {},
   "source": [
    "## PRODUCTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products_raw = df_products\n",
    "df_products_mod=df_products_raw.copy().drop_duplicates()\n",
    "\n",
    "\n",
    "'''COLONNA CATEGORY'''\n",
    "#METODO LEV CON SOGLIA 6\n",
    "df_products_cat_lev_6, dict_products_cat=pulisci_stringhe_lev(df_products_mod,'Category',6)\n",
    "df_products_cat_corrette=pd.DataFrame([(k, v) for k, v in dict_products_cat.items()  if len(v)>1], columns=['Category', 'Variants'])\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"Using levenshtein function with threshold=6, the 'cleaning' dataframe is: \")\n",
    "    display(df_products_cat_corrette)\n",
    "    verbose2(dict_products_cat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49eff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA SUB-CATEGORY'''\n",
    "#METODO LEV CON SOGLIA 2\n",
    "df_products_sub_lev2, dict_products_sub=pulisci_stringhe_lev(df_products_cat_lev_6,'Sub-Category',2)\n",
    "df_products_sub_corrette=pd.DataFrame([(k, v) for k, v in dict_products_sub.items() if len(v)>1], columns=['Sub-Category', 'Variants'])\n",
    "\n",
    "\n",
    "df_products_sub_lev2=df_products_sub_lev2[['Product ID','Category','Category_mod','Sub-Category','Sub-Category_mod','Product Name']]#cambio ordine delle colonnne\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"Using levenshtein function with threshold=2, the 'cleaning' dataframe is: \")\n",
    "    display(df_products_sub_corrette)\n",
    "    verbose2(dict_products_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''COLONNA PRODUCT-ID'''\n",
    "if verbose >= 1:\n",
    "    print(\"In this section, the structure of 'Product ID' is validated based on 'Category' and 'Sub-Category'.\")\n",
    "    print(\"Expected format: the ID should be composed as 'XXX-YY' where:\")\n",
    "    print(\"- 'XXX' matches the first 3 letters of 'Category'\")\n",
    "    print(\"- 'YY' matches the first 2 letters of 'Sub-Category'\")\n",
    "    \n",
    "\n",
    "df_products_sub_lev2['Product ID_mod']=df_products_sub_lev2['Product ID']\n",
    "\n",
    "for index, row in df_products_sub_lev2.iterrows():\n",
    "    product_id = row['Product ID_mod']\n",
    "    category = row['Category_mod']\n",
    "    sub_category= row['Sub-Category_mod']\n",
    "\n",
    "    if pd.notnull(product_id) and '-' in product_id:\n",
    "        product_cat = product_id.split('-')[0]\n",
    "        product_sub_cat = product_id.split('-')[1]\n",
    "\n",
    "        if product_cat != category[:3]:  # primi 3 caratteri della stringa\n",
    "            print(f'La riga {index} è sbagliata: ID = {product_id}, Categoria = {category}')\n",
    "        \n",
    "        elif product_sub_cat != sub_category[:2]:\n",
    "            print(f'La riga {index} è sbagliata: ID= {product_id}, Sub-Category= {product_sub_cat}')\n",
    "            #OK NO ERRORI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8a82c",
   "metadata": {},
   "source": [
    "## CUSTOMERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e834bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers_raw=df_customers\n",
    "df_customers_mod=df_customers_raw.copy().drop_duplicates()\n",
    "\n",
    "'''COLONNA SEGMENT'''\n",
    "#METODO LEV CON SOGLIA 2\n",
    "df_customers_lev_2, dict_segment=pulisci_stringhe_lev(df_customers_mod,'Segment',2)\n",
    "df_customers_segment_corretto=pd.DataFrame([(k, v) for k, v in dict_segment.items() if len(v)>1], columns=['Segment', 'Variants'])\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"Using levenshtein function with threshold=2, the 'cleaning' dataframe is: \")\n",
    "    display(df_customers_segment_corretto)\n",
    "    verbose2(dict_segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f93802",
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose >= 1:\n",
    "    print(\"In this section, Customer IDs are validated and corrected based on Customer Names.\")\n",
    "    print(\"Steps performed:\")\n",
    "    print(\"- Hyphens (`-`) and underscores (`_`) in 'Customer Name' are replaced with spaces.\")\n",
    "    print(\"- Each 'Customer ID' is expected to have the first letter of the name and surname.\")\n",
    "    print(\"- If the initials in the ID do not match the name, the ID is corrected accordingly.\")\n",
    "    print(\"- Some compound names (e.g., 'CoreyLock') are split using capital letters to extract initials.\")\n",
    "    \n",
    "\n",
    "df_customers_lev_2['Customer ID_mod']=df_customers_lev_2['Customer ID'].copy()\n",
    "df_customers_lev_2['Customer Name_mod'] = (df_customers_lev_2['Customer Name'].str.replace('-', ' ', regex=False).str.replace('_', ' ', regex=False))\n",
    "\n",
    "'''COLONNA CUSTOMER ID'''\n",
    "for index in df_customers_lev_2.index:\n",
    "    customer_id = df_customers_lev_2.at[index, 'Customer ID_mod']\n",
    "    customer_full_name = df_customers_lev_2.at[index, 'Customer Name_mod']\n",
    "    \n",
    "    if pd.notnull(customer_id) and '-' in customer_id:\n",
    "        customer_id_first_letter_name = customer_id[0]\n",
    "        customer_id_first_letter_surname = customer_id[1]\n",
    "\n",
    "        parts = customer_full_name.strip().split()\n",
    "        if len(parts) >= 2:\n",
    "            first_letter_name = parts[0][0]\n",
    "            first_letter_surname = parts[1][0]\n",
    "        else:\n",
    "            # Gestione nome-cognome attaccati tipo 'CoreyLock'\n",
    "            split_parts = re.split(r'(?=[A-Z])', customer_full_name)\n",
    "            if len(split_parts) >= 3:\n",
    "                first_letter_name = split_parts[1][0]\n",
    "                first_letter_surname = split_parts[2][0]\n",
    "            else:\n",
    "                continue  # nome non gestibile, salta\n",
    "\n",
    "        if (\n",
    "            customer_id_first_letter_name != first_letter_name or\n",
    "            customer_id_first_letter_surname != first_letter_surname\n",
    "        ):\n",
    "            print(f'La riga {index} è errata: il Customer ID è {customer_id}, il Customer Name è {customer_full_name}')\n",
    "            df_customers_lev_2.at[index, 'Customer ID_mod'] = first_letter_name + first_letter_surname + customer_id[2:]\n",
    "\n",
    "dict_customers_id_corretti=creo_dict(df_customers_lev_2,'Customer ID','Customer ID_mod')\n",
    "df_customers_id_corretti=pd.DataFrame([(k, v) for k, v in dict_customers_id_corretti.items() if len(v)>1], columns=['Customer ID', 'Variants'])\n",
    "\n",
    "df_customers_lev_2=df_customers_lev_2.rename(columns={'Customer ID_mod':'Customer ID_def'})\n",
    "if verbose==2:\n",
    "    print(\"The 'cleaning' dataframe is: \")\n",
    "    display(df_customers_id_corretti)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a32b7ae",
   "metadata": {},
   "source": [
    "# MERGE TRA TUTTI I DATAFRAME\n",
    "- while merging dataframes, I drop non-standardized or uncleaned columns to keep only the cleaned ones.\n",
    "- after merging, I rename the cleaned columns from '*_mod' to '*_def' to mark them as final versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943fc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGE TRA ORDERS E COUNTRIES SULLE CITTA'\n",
    "if verbose >= 1:\n",
    "    print(\"Merging the orders and countries dataframes using ['City_def', 'Postal Code_mod'] as keys.\")\n",
    "\n",
    "\n",
    "df_date_pulite2['Postal Code_mod'] = df_date_pulite2['Postal Code'].astype(str)\n",
    "df_countries_fuzz_city_95['Postal Code_mod'] = df_countries_fuzz_city_95['Postal Code'].astype(str)\n",
    "\n",
    "df_merge = pd.merge(df_date_pulite2.drop(columns=['Order Date','Ship Date','Ship Mode','Postal Code','Sales','Quantity','Discount','Profit','City_fuzz_90','City_fuzz_85','City','Product ID']), df_countries_fuzz_city_95.drop(columns=['City','Postal Code','State_fuzz_90','State_fuzz_80','State','City_fuzz_90']), how='left', on=(['City_def','Postal Code_mod']))\n",
    "pd.set_option('display.max_columns', None) #visualizzo tutte le colonne dei df\n",
    "\n",
    "\n",
    "df_merge=df_merge[['Order ID','Product ID_def','Customer ID','Country','Region','State_lev_1','City_def','Postal Code_mod','Order Date_def','Ship Date_def','Ship Mode_mod','Sales_mod','Profit_mod','Discount_mod','Quantity_mod']]\n",
    "df_merge=df_merge.rename(columns={'State_lev_1':'State_def',\n",
    "                                  'Customer ID':'Customer ID_def',\n",
    "                                  'Postal Code_mod':'Postal Code_def',\n",
    "                                  'Profit_mod':'Profit_def',\n",
    "                                  'Sales_mod':'Sales_def',\n",
    "                                  'Ship Mode_mod':'Ship Mode_def',\n",
    "                                  'Discount_mod':'Discount_def',\n",
    "                                  'Quantity_mod':'Quantity_def',\n",
    "                                  })\n",
    "\n",
    "if verbose==2:\n",
    "    print(\"The merged dataframe is\")\n",
    "    display(df_merge.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f417ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGE TRA IL DF CON GIA ORDERS E COUNTRIES E IL DF CUSTOMERS CON CHIAVE CUSTOMER ID_DEF\n",
    "if verbose>=1:\n",
    "    print(\"Merging customers dataframe with the orders-countries one, using 'Customer ID_def' as key\")\n",
    "\n",
    "df_merge_customers=df_merge.merge(df_customers_lev_2.drop(columns=['Segment','Customer ID','Customer Name']), on='Customer ID_def', how='left')\n",
    "df_merge_customers=df_merge_customers.rename(columns={'Segment_mod':'Segment_def',\n",
    "                                                      'Customer Name_mod':'Customer Name_def'})\n",
    "\n",
    "if verbose==2:\n",
    "    print('The merged dataframe is:')\n",
    "    display(df_merge_customers.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cee043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGE TRA DF GIA CON I 3 FOGLI E IL DF PRODUCTS CON CHIAVE PRODUCT ID_DEF\n",
    "if verbose>=1:\n",
    "    print(\"Merging orders-countries-customers dataframe with products one, using 'Porduct ID_def' as key\")\n",
    "    print(\"Cleaning malformed 'Product ID' values using reconstruction logic for missing category data.\")\n",
    "    print(\"if the category starts with 'OF' but is not 'OFF', then i add a 'F' and then go back to standardized format \")\n",
    "    print(\"Then, I replace manually some others incorrect Product IDs (hardcoded corrections).\")\n",
    "\n",
    "\n",
    "df_products_sub_lev2=df_products_sub_lev2.rename(columns={'Product ID_mod':'Product ID_def'})\n",
    "\n",
    "df_merge_def=df_merge_customers.merge(df_products_sub_lev2.drop(columns=['Product Name','Category','Sub-Category','Product ID']).drop_duplicates(), on='Product ID_def',how='left')\n",
    "\n",
    "\n",
    "'''PULISCO ERRORI SU PRODUCT ID'''\n",
    "df_merge_def=df_merge_def.rename(columns={'Product ID_def':'Product ID_mod'})\n",
    "df_merge_def['Product ID']=df_merge_def['Product ID_mod'].copy()\n",
    "\n",
    "\n",
    "for index, row in df_merge_def.iterrows():\n",
    "    if pd.isna(row['Category_mod']):\n",
    "        product_parts=row['Product ID_mod'].split('-')\n",
    "        categoria=product_parts[0]\n",
    "        sotto_categoria=product_parts[1]\n",
    "        codice_numerico=product_parts[2]\n",
    "        if categoria.startswith('OF') and categoria != 'OFF' and len(categoria) == 3:\n",
    "            nuova_categoria=categoria[0]+'F'+categoria[1]\n",
    "            ultima_lettera_categoria=categoria[2]\n",
    "            \n",
    "            nuova_sotto_categoria=ultima_lettera_categoria + sotto_categoria[:-1]\n",
    "            ultima_lettera_sotto_categoria= sotto_categoria[-1]\n",
    "\n",
    "            nuovo_codice_numerico=ultima_lettera_sotto_categoria + codice_numerico\n",
    "\n",
    "            nuovo_product_id = f\"{nuova_categoria}-{nuova_sotto_categoria}-{nuovo_codice_numerico}\"\n",
    "\n",
    "            # Applica la modifica\n",
    "            df_merge_def.at[index, 'Product ID_mod'] = nuovo_product_id\n",
    "\n",
    "#FACCIO IL REPLACE PER ERRORI CHE NON SO COME CORREGGERE\n",
    "df_merge_def['Product ID_mod']=df_merge_def['Product ID_mod'].replace('FUR-BO-10000468','FUR-BO-1000468').replace('FUR-U1-0001935','FUR-FU-10001935').replace('OFF-BI-1000948','OFF-BI-10000948').replace('OFF-ST-10001128','OFF-ST-10001228').replace('OFF-21-0002049','OFF-BI-10002049').replace('FUR-BO-10002206','FUR-BO-1002206').replace('TEC-PH-O10002555','TEC-PH-10002555').replace('OFF-PA-1001274','OFF-PA-10001274').replace('OFF-PA-1003724','OFF-PA-10003724')\n",
    "\n",
    "df_merge_def=df_merge_def.rename(columns={'Product ID_mod':'Product ID_def'})\n",
    "df_merge_def=df_merge_def.drop(columns=['Product ID','Category_mod','Sub-Category_mod']).merge(df_products_sub_lev2.drop(columns=['Product Name','Category','Sub-Category','Product ID']).drop_duplicates(), on='Product ID_def',how='left')\n",
    "df_merge_def=df_merge_def.rename(columns={'Category_mod':'Category_def', 'Sub-Category_mod':'Sub-Category_def'})\n",
    "\n",
    "if verbose==2:\n",
    "    print('The merged dataframe is:')\n",
    "    display(df_merge_def.sample(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b3242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  4.,  3.,  2.,  0.,  7., nan,  1.,  6.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''AGGIUNGO COLONNA MESE, QUARTER, ANNO, DISTANZA IN GIORNI TRA SHIP DATE E ORDER DATE, TIPO DI GIORNO DELL'ORDINE(GIORNO DELLA SETTIMANA O DEL WEEKEND)'''\n",
    "df_merge_def['Order Month'] = df_merge_def['Order Date_def'].dt.month_name()\n",
    "df_merge_def['Order Year'] = df_merge_def['Order Date_def'].dt.year\n",
    "df_merge_def['Order Quarter']=df_merge_def['Order Date_def'].dt.quarter\n",
    "df_merge_def['Days between order date and shipping date']= df_merge_def['Ship Date_def'] - df_merge_def['Order Date_def']\n",
    "df_merge_def['Order Day Type'] = df_merge_def['Order Date_def'].dt.dayofweek.apply(\n",
    "    lambda x: 'Weekend' if x >= 5 else 'Weekday')\n",
    "df_merge_def['Order Week']=df_merge_def['Order Date_def'].dt.isocalendar().week\n",
    "df_merge_def['Order Day of the Week']=df_merge_def['Order Date_def'].dt.day_name()\n",
    "#display(df_merge_def['Region'].unique())\n",
    "#display(df_merge_def.loc[df_merge_def['Region'].isna()])\n",
    "\n",
    "'''RENDO LE COLONNE TESTUALI DA TIPO OBJECT A TIPO STR'''\n",
    "df_merge_def['Days between order date and shipping date']=df_merge_def['Days between order date and shipping date'].dt.days\n",
    "display(df_merge_def['Days between order date and shipping date'].unique())\n",
    "display(df_merge_def.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1e539",
   "metadata": {},
   "source": [
    "CREO CSV CON IL DF FINALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b124d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_def.to_csv('../data/processed/MasterTable.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
